# 卷积神经网络（Convolutional Neural Networks, CNN）

## 一、概念
* 一类包含卷积计算且具有深度结构的**前馈神经网络**（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一。
* 多层感知机(MLP)的变种，本质是一个多层感知机。成功的原因在于其所采用的局部连接和权值共享的方式：一方面减少了权值的数量使得网络易于优化；另一方面降低了模型的复杂度、减小了过拟合的风险。

## 二、特点：
1. 各层中的神经元是3维排列的：宽度、高度和深度。
![minerU](./pictures/CNN1.jpg)
2. 对于输入层：宽度和高度指的是输入图像的宽度和高度，深度代表输入图像的通道数
3. 对于中间层：宽度和高度指的是特征图(feature map)的宽和高，通常由卷积运算和池化操作的相关参数决定；深度指的是特征图的通道数，通常由卷积核的个数决定。

## 三、基本概念
### 3.1 卷积
#### 3.1.1 卷积和卷积核
* 卷积操作其实就是每次取一个特定大小的矩阵**F**（卷积核，即convolutional kernel或**kernel**或**filter**或detector，它可以是一个也可以是多个），然后将其对输入**X**（图中蓝色矩阵）依次扫描并进行内积的运算过程。卷积后的结果**Y**称为特征图(feature map)。
* 卷积操作可以看作是对上一次输入的特征提取，即用来抓取输入中是否包含有某一类的特征。
#### 3.1.2 卷积的计算
##### 单通道单卷积核
得到右边形状为[3,3,1]的卷积特征图
|![minerU](./pictures/CNN2.jpg)|![minerU](./pictures/CNN3.jpg)|
|-|-|
|![minerU](./pictures/CNN1.png)|
##### 单通道多卷积核
得到右边形状为[3,3,2]的卷积特征图
|![minerU](./pictures/CNN4.jpg)|![minerU](./pictures/CNN6.jpg)|![minerU](./pictures/CNN5.jpg)|
|-|-|-|
##### 多通道单卷积核
得到右边形状为[3,3,1]的卷积特征图
|![minerU](./pictures/CNN7.jpg)|![minerU](./pictures/CNN8.jpg)|
|-|-|
|![minerU](./pictures/CNN2.png)|
##### 多通道多卷积核
得到右边形状为[3,3,2]的卷积特征图
|![minerU](./pictures/CNN9.jpg)|![minerU](./pictures/CNN10.jpg)|
|-|-|
##### 总结
1. 原始输入有多少个通道，其对应的一个卷积核就必须要有多少个通道。
2. 用k个卷积核对输入进行卷积处理，那么最后得到的特征图一定就会包含有k个通道。
#### 3.1.3 深度卷积
深度卷积就是卷积之后再卷积，然后再卷积。，卷积次数不限。
##### 作用
1. 通常情况下，输入的图像数据都是由一系列特征横向和纵向组合叠加起来的。因此，对于同一层次（横向）的特征我们需要通过多个卷积核对输入进行特征提取；而对于不同层次 **（纵向）的特征** 需要通过**卷积的叠加**来进行特征提取。
2. 对于输入的一张图片，我们可以通过取多次叠加卷积后的结果来进行物体的分类任务。
##### 应用场景
1. 不仅仅是用于图像的特征提取，在**相邻空间位置上具有依赖关系的数据**均可以通过卷积操作来进行特征提取。
2. 图像数据最重要的属性就是相邻位置上的像素之存在着空间上的依赖（space-correlation ）关系。
#### 3.1.4 总结
在卷积神经网络中，对于输入的图像
1. 需要多个不同的卷积核对其进行卷积，来提取这张图像不同的特征（多核卷积）。
2. 同时也需要多个卷积层进行卷积，来提取深层次的特征（深度卷积）。
### 3.2 感受野(Receptive Field)
1. 感受野指的是卷积神经网络每一层输出的特征图(feature map)上每个像素点映射回输入图像上的区域大小。
2.  **感受野的范围可以用来大致判断每一层的抽象层次。** 神经元感受野的范围越大表示其能接触到的原始图像范围越大，也意味着它能学习更为全局，语义层次更高的特征信息；相反，范围越小则表示其所包含的特征越趋向局部和细节。**网络越深，神经元的感受野越大。**

    ![minerU](./pictures/CNN11.jpg)
    从输入特征图到输出特征图尺寸的计算公式（其中$n_{in}$为输入size，$p$为padding大小，$f$为卷积核size，$s$为卷积步长）
    假设输入大小为5×5，f=3×3，padding为1×1，卷积步长为2×2，那么输出特征图size根据公式可计算为3×3。
    ![minerU](./pictures/CNN12.jpg)
    感受野的计算公式（其中$RF_{l+1}$为当前特征图对应的感受野大小，也就是我们要计算的目标感受野，$RF_l$为上一层特征图对应的感受野大小，$f_{l+1}$为当前卷积层卷积核大小，最后一项连乘项则表示之前卷积层的步长乘积。）
### 3.3 权值共享
在一个卷积核在和一个n通道的特征图进行卷积运算时，可以看作是用这个卷积核作为一个滑块去“扫”这个特征图，卷积核里面的数就叫权重，这个特征图每个位置是被同样的卷积核“扫”的，所以权重是一样的，也就是共享。
### 3.4 分辨率(Resolution)
* 分辨率是输入模型的图像尺寸，即长宽大小。根据模型下采样次数n和最后一次下采样后feature map的分辨率k×k来决定。
* 从输入r×r到最后一个卷积特征feature map的k×k，整个过程是一个信息逐渐抽象化的过程，即网络学习到的信息逐渐由低级的几何信息转变为高级的语义信息。
* k太大会增加后续的计算量且信息抽象层次不够高，影响网络性能，k太小会造成非常严重的信息丢失。
### 3.5 网络深度(Depth)
神经网络的深度决定了网络的表达能力，早期的backbone设计都是直接使用**卷积层堆叠**的方式，它的深度即神经网络的层数；后来的backbone设计采用了更高效的**module(或block)堆叠**的方式，每个module是由多个卷积层组成，它的深度也可以指module的个数。
### 3.6 网络宽度(Width)
网络的宽度指的是卷积神经网络中最大的通道数，由卷积核数量最多的层决定。
### 3.7 下采样(Down-Sample)
1. 作用：
    1. 减少计算量，防止过拟合
    2. 增大感受野，使得后面的卷积核能够学到更加全局的信息
1. 设计（两种）：
   1. 采用步长(stride)为2的池化层，如Max-pooling或Average-pooling，目前通常使用Max-pooling，因为它**计算简单且最大响应能更好保留纹理特征**。
   2.  采用步长(stride)为2的卷积层，下采样的过程是一个信息损失的过程，而池化层是不可学习的，用stride为2的**可学习**卷积层来代替pooling可以得到更好的效果，当然同时也增加了一定的计算量。
### 3.8 上采样(Up-Sampling)
输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸会变小，而我们需要将图像恢复到原来的尺寸以便进行进一步的计算。
使图像由小分辨率映射到大分辨率的操作。
实现方法：
 * **插值**，一般使用的是双线性插值，因为效果最好，虽然计算上比其他插值方式复杂，但是相对于卷积计算可以说不值一提；
* **转置卷积**又或是说反卷积，通过对输入feature map间隔填充0，再进行标准的卷积计算，可以使得输出feature map的尺寸比输入更大；
* **Max Unpooling**，在对称的max pooling位置记录最大值的索引位置，然后在unpooling阶段时将对应的值放置到原先最大值位置，其余位置补0。
### 3.9 参数量(Params)
参数量指的**网络中可学习变量的数量**。包括卷积核的权重weight，批归一化(BN)的缩放系数γ，偏移系数β，有些没有BN的层可能有偏置bias，这些都是可学习的参数，即在模型训练开始前被赋予初值，在训练过程根据链式法则中不断迭代更新。
整个模型的参数量主要由卷积核的权重weight的数量决定，参数量越大，则该结构对运行平台的内存要求越高，参数量的大小是轻量化网络设计的一个重要评价指标。
### 3.10 计算量(FLOPs)
神经网络的前向推理过程基本上都是乘累加计算，所以它的计算量也是指的**前向推理过程中乘加运算的次数**，通常用FLOPs(即“每秒浮点运算次数”)来表示，即floating point operations(浮点运算数)。

## 四、卷积结构类型
卷积结构是CNN的重要组成单元。
### 4.1 标准卷积 (Convolution)
即3.1.2写的。标准卷积中每个卷积核都需要与feature map的所有通道进行计算，所以每个卷积核的通道数等于输入feature map的通道数，同时通过设定卷积核的数量可以控制输出feature map的通道数。

- **定义**：最常见的卷积结构，每个卷积核会同时作用于输入特征图的所有通道。  
- **输入输出关系**：  
  - 输入特征图维度：\((1, iC, iH, iW)\)  
  - 卷积核维度：\((1, iC, k, k)\)  
  - 输出特征图维度：\((1, oC, oH, oW)\)，其中 \(oC\) 是卷积核数量。  
- **计算量**：公式为  
  \[
  iC \times k \times k \times oC \times oH \times oW
  \]  
  即通道数 × 卷积核大小 × 卷积核数量 × 输出空间大小。  
- **特点**：能提取跨通道的综合特征，但计算量和参数量较大。

### 4.2 深度卷积 (Depthwise Convolution)
- **定义**：与标准卷积不同，每个卷积核只作用于输入特征图的单个通道，而不是所有通道。  
- **输入输出关系**：  
  - 卷积核维度：\((1, 1, k, k)\)，每个卷积核只对应一个通道。  
  - 卷积核数量：等于输入通道数 \(iC\)。  
  - 输出维度：\((1, iC, oH, oW)\)，通道数保持不变。  
- **不足**：不能改变输出通道数，因此通常在深度卷积后接一个 \(1 \times 1\) 的标准卷积（Pointwise Convolution），形成 **深度可分离卷积 (Depthwise Separable Convolution)**。  
- **计算量**：  
  \[
  k \times k \times iC \times oH \times oW + iC \times 1 \times 1 \times oH \times oW \times oC
  \]  
  相比标准卷积大幅减少，约为普通卷积的 \(\frac{1}{oC} + \frac{1}{k^2}\)。  
- **应用**：这种结构在 **MobileNet V1** 中提出，成为轻量化网络设计的核心方法。
✅ 总结：标准卷积是“所有通道一起处理”，功能强但耗资源； 深度卷积是“每个通道单独处理”，省资源但需要后续融合。
好的，那我来以更学术化、技术化的方式总结这七类卷积结构，不做通俗化比喻：

### 4.3 分组卷积 (Group Convolution)
（顾名思义）
- **定义**：将输入特征图分为 \(g\) 组，每组独立进行卷积。  
- **输出**：各组结果拼接为 \((1, oC, oH, oW)\)。  
- **计算量**：iC/g×k×k×oC×oH×oW。为标准卷积的 \(1/g\)，参数量亦为 \(1/g\)。  
- **问题**：组间信息隔离，需通过 **channel shuffle** 操作增强信息交互。  
- **应用**：AlexNet 首次提出，ShuffleNet进一步优化。

### 4.4  空洞卷积 (Dilated Convolution)
（扩张卷积或者膨胀卷积）
- **定义**：在卷积核中插入空格或在特征图上间隔采样。  
- **特点**：在不增加参数量的情况下**扩大感受野**。  
- **应用**：针对图像语义分割问题中下采样会降低图像分辨率、丢失信息的问题。  
- **参数**：扩张率 (dilation rate) 控制采样间隔，标准卷积对应扩张率为 1。

### 4.5 转置卷积 (Transposed Convolution)
（反卷积(Deconvolution)）
- **目的**：为上采样而生，也应用于语义分割当中
- **定义**：通过在输入特征图中插入零，再进行标准卷积计算。  
- **特点**：实现上采样，输出特征图尺寸增大。  
- **应用**：语义分割、生成模型（如 GAN）中常用。


### 4.6 可变形卷积 (Deformable Convolution)
- **定义**：在卷积核采样点上引入在h和w方向上可学习的偏移量，使采样位置动态调整。  
- **特点**：卷积核可根据输入特征自适应选择采样点，提升对不规则形状的建模能力。  
- **应用**：复杂场景下的目标检测与分割，增强特征提取灵活性。

### 4.7 1×1卷积 (Pointwise Convolution)
- **定义**：卷积核大小为 \(1 \times 1\)，仅在通道维度上进行加权。  
- **作用**：  
  - **降维**：减少通道数，降低计算量。  
  - **升维**：增加通道数，增强表达能力。  
  - **增加非线性**：结合激活函数提升网络复杂度。  
- **应用**：Network in Network 首次提出，后广泛应用于 Inception、ResNet 等结构。

## 五、卷积神经网络组成

## 参考学习链接
1. （2023-12-11）[ 【综述】一文读懂卷积神经网络(CNN) ](https://zhuanlan.zhihu.com/p/561991816)
2. （2022-08-26） [原来卷积是这么计算的](https://zhuanlan.zhihu.com/p/268179286?ivk_sa=1024320u)