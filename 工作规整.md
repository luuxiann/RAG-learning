## 一、相关模型调研
测评效果的比对主要采用手头所有的经过挑选的有代表性的共44页6组投标文档，人工进行比对。
### 1.1  MonkeyOCR
 https://github.com/Yuliang-Liu/MonkeyOCR
#### 1.1.1  MonkeyOCR
Demo：http://vlrlabmonkey.xyz:8891
部分解析效果展示：
 
| ![MonkeyOCR](./pictures/Mo01.png)|![MonkeyOCR](./pictures/Mo02.png)|
|--|--|
 | ![MonkeyOCR](./pictures/Mo04.png)|![MonkeyOCR](./pictures/Mo03.png)|
| ![MonkeyOCR](./pictures/Mo05.png)|![MonkeyOCR](./pictures/Mo08.png)|
| ![MonkeyOCR](./pictures/Mo07.png)|![MonkeyOCR](./pictures/Mo06.png)|
 
平均解析速度为3.2s/页
MonkeyOCR在用以测试的投标文件中表现较差，出现内容缺失，页眉跨页表格错误分类，表格覆盖不全的情况，问题较多，除了解析速度并未发现其他比MinerU2.5的vlm解析效果好的地方，故不予采用。
#### 1.1.2  MonkeyOCR v1.5
Demo:https://aiwrite.wps.cn/pdf/parse/web/
11月中旬出的，解析效果较MonkeyOCR有了很大提升，在页眉标题识别这一块的效果优于MinerU2.5和Dolphin v2。
部分解析效果展示（2026.01.22重新测评）：
| ![MonkeyOCR v1.5](./pictures/Mo3.png)|![MonkeyOCR v1.5](./pictures/Mo8.png)|
|--|--|
| ![MonkeyOCR v1.5](./pictures/Mo7.png)|![MonkeyOCR v1.5](./pictures/Mo2.png)|
| ![MonkeyOCR v1.5](./pictures/Mo4.png)|![MonkeyOCR v1.5](./pictures/Mo5.png)|
| ![MonkeyOCR v1.5](./pictures/Mo10.png)|![MonkeyOCR v1.5](./pictures/Mo11.png)|
| ![MonkeyOCR v1.5](./pictures/Mo9.png)|![MonkeyOCR v1.5](./pictures/Mo1.png)|
| ![MonkeyOCR v1.5](./pictures/Mo6.png)|

平均解析速度为3.1s/页
分类效果除表格外都没出问题，边界框覆盖很全面，整体效果比MinerU2.5的vlm还好，但跨页表格分类还是不准确，不能进行跨页合并。
MonkeyOCR v1.5出来时demo和代码似乎未更新完全，当时测验效果跟MoonkeyOCR一致，故后续优化MinerU2.5时未考虑该模型。
### 1.2 FluxOCR
Demo：https://ocrflux.pdfparser.io/#/
简单展示用Demo测验的效果。Demo只支持3页，且每日解析数量有上限，所以体现的问题并不全面。
部分解析效果展示：
| ![FluxOCR](./pictures/F1.png)|![FluxOCR](./pictures/F2.png)|
|--|--|
|![FluxOCR](./pictures/F3.png)|![FluxOCR](./pictures/F4.png)|

解析速度也在3s/页，跨页表格、页眉分类错误，分级有时会出错。跨页表格合并效果不错，但有时又不能合并。

###  1.3  Logics-Parsing 
https://github.com/alibaba/Logics-Parsing
Demo：https://www.modelscope.cn/studios/Alibaba-DT/Logics-Parsing/summary
阿里的，文章里写的是比MinerU2好
1. 部分解析效果展示：
    | ![Logics-Parsing](./pictures/L1.png)|![Logics-Parsing](./pictures/L5.png)|
    |--|--|
    | ![Logics-Parsing](./pictures/L3.png)|![Logics-Parsing](./pictures/L4.png)|
    | ![Logics-Parsing](./pictures/L2.png)|![Logics-Parsing](./pictures/L7.png)|
    | ![Logics-Parsing](./pictures/L9.png)|![Logics-Parsing](./pictures/L8.png)|
    | ![Logics-Parsing](./pictures/L10.png)|![Logics-Parsing](./pictures/L11.png)|
    | ![Logics-Parsing](./pictures/L6.png)|![Logics-Parsing](./pictures/L12.png)|

    解析速度在2.6s/页。基本无大问题，页眉分类结果不稳定，一次能分类出来再识别一次就分类错误了（见最后两张），有的会直接错误分类为页眉（见第7、8张）。因为用的Demo网页，所以倒数第4张的表格转换后看起来空白单元格还在（见倒数第3张），实际Markdown里是没有的。
1.  技术原理:
* 基于 Qwen2.5-VL-7B：Logics-Parsing 以强大的 Qwen2.5-VL-7B 模型为基础，继承了其在视觉和语言处理方面的优势。
* 两阶段训练：第一阶段是监督微调，模型学习生成结构化的 HTML 输出；第二阶段是布局为中心的强化学习，通过文本准确性、布局定位和阅读顺序三个奖励组件优化模型。
* 强化学习优化：通过强化学习，模型能更好地理解文档的布局和内容的逻辑顺序，生成更准确的结构化输出。
*  结构化 HTML 输出：模型能将文档图像转换为结构化的 HTML 格式，保留文档的逻辑结构，每个内容块都有类别、边界框坐标和 OCR 文本标签。
* 高级内容识别：模型能准确识别复杂科学公式、化学结构和手写中文字符，将化学结构转换为标准的 SMILES 格式。
* 自动去除无关元素：模型能自动识别并过滤掉页眉、页脚等无关元素，专注于文档的核心内容。
### 1.4 chandra 
https://github.com/datalab-to/chandra
Demo：https://www.datalab.to/playground
布局分析结果和MinerU的vlm效果一样（Demo演示，只能用基础版的，他还有一个更精确的），跨页表格还是会分类错误。
表格markdown输出直接用的markdown格式而非html，呈现效果一般
目前标题分级里做的最好的了，几乎没有把正文误识别为标题的情况。

### 1.5  Paddlex
https://huggingface.co/PaddlePaddle/PaddleOCR-VL
#### 1.5.1  [PaddleOCR-VL](https://github.com/PaddlePaddle/PaddleOCR)
Demo：https://aistudio.baidu.com/paddleocr
1. 两阶段方案，由layout+VLM组成:
   * 阶段1：PP-DocLayoutV2，负责布局分析，定位语义区域并预测其阅读顺序。PP-DocLayoutV2结构式RT-DETR以及一个具有六个transformer层的轻量级指针网络，以准确预测布局元素的阅读顺序。
   * 阶段2：PaddleOCR-VL-0.9B对文本、表格、公式和图表进行ocr format。模型结构类似LLaVA：
     * 视觉编码器：使用NaViT结构，从keye-vl初始化，支持原生分辨率输入（任意分辨率的图像而不会失真，从而减少幻觉）。
     * 连接器：随机初始化的2层MLP
     * 解码器：ERNIE-4.5-0.3B，引入3D-RoPE进一步增强了位置表示
 
2. [再看两阶段多模态文档解析大模型-PaddleOCR-VL架构、数据、训练方法](https://zhuanlan.zhihu.com/p/1962581920517986232)
        其中的第一阶段布局解析所用工具：[PP-DocLayoutV3](https://www.paddleocr.ai/main/version3.x/pipeline_usage/PP-StructureV3.html)
    https://zhuanlan.zhihu.com/p/1887627016414664307
    这个用解析效果不好，表格输出不换行，会把正文里的一些数字识别成公式。
#### 1.5.2 PP-StructureV3
同一个团队的，专门针对复杂文档的
Demo：https://www.modelscope.cn/studios/PaddlePaddle/PP-StructureV3_Online_Demo
这个演示用不，下代码看
处理速度 5s/页。效果上小问题比较多:
1. 表格识别上，定位分类没问题，未出现右下角日期签名分类错误情况，但是覆盖和具体内容识别会出问题。
2. 标题分级上的效果不理想，依旧有出现把正文内容识别为标题的情况
3. 文字识别会有缺漏的情况
4. 第五张图直接整个表格错乱了，不知是布局分析问题还是表格识别问题
5. 针对两栏的阅读顺序的编排上有些问题
部分解析效果展示：

|![PP-StructureV3](./pictures/pp%20v3%201.png)|![PP-StructureV3](./pictures/pp%20v3%202.png)|
|--|--|
|![PP-StructureV3](./pictures/pp%20v3%203.png)|![PP-StructureV3](./pictures/pp%20v3%205.png)|
|![PP-StructureV3](./pictures/pp%20v3%204.png)|
### 1.6 olmOCR
[olmOCR](https://github.com/allenai/olmocr)
Demo：https://olmocr.allenai.org/
主要用于简单布局的文档，在简单布局的PDF上的表现确实比市面上同参数模型要优秀一点。主要还是想看看其在表格上的效果，不过还是一般。
处理速度：0.8s/页，少见的连横线都能给你识别还原出来。不看表格，分类识别效果还是不错的。没出现像minerU的vlm那样把左上角的附件介绍文本当作页眉的情况。
嵌套表格识别效果挺不错：
![olmOCR](./pictures/olmOCR1.png)
下一页就不太理想了：
![olmOCR](./pictures/olmOCR2.png)
技术原理
   * 文档锚定（Document-anchoring）：基与提取 PDF 页面中的文本块和图像的位置信息，与原始文本结合，形成提示（prompt）。提示与页面的栅格化图像一起输入到视觉语言模型（VLM）中。帮助模型更好地理解文档的结构和布局，减少因图像模糊或布局复杂导致的提取错误。
   * 微调的视觉语言模型（VLM）：基于 Qwen2-VL-7B-Instruct 的 7B 参数视觉语言模型。在包含 26 万页 PDF 的数据集上进行微调，适应文档处理任务。模型输出结构化的 JSON 数据，包含页面的元数据（如语言、方向、是否包含表格等）和自然阅读顺序的文本内容。
   * 高效推理与成本优化：用 SGLang 和 vLLM 等高效推理框架，支持大规模并行处理。基于优化硬件利用和推理流程，olmOCR 的处理成本极低，每百万页仅需 190 美元，远低于其他商业解决方案。
   * 鲁棒性增强：在遇到提取失败或重复生成时，自动重试并调整提示内容。自动检测页面方向并进行旋转校正，确保内容正确提取。

## 二、MinerU2.5
https://github.com/opendatalab/MinerU
Demo:https://mineru.net/OpenSourceTools/Extractor?source=github

####  2.1 整体架构
模型基于 Qwen2-VL 框架优化，由三大核心模块组成，总参数量仅 1.2B，兼顾轻量化与高性能：
* **视觉编码器**：基于 **2B参数的Qwen2-VL-Instruct 初始化**，替换窗口注意力为全局自适应注意力，避免文档解析中细节丢失。675M 参数的 **NaViT 模型**，支持**动态分辨率输入**，采用 **2D-RoPE 位置编码**，适配不同尺寸和宽高比的文档区域。
* **语言解码器**：0.5B 参数的 Qwen2-Instruct，替换 1D-RoPE 为 M-RoPE，增强对不同分辨率裁剪区域的泛化能力。
* **Patch Merger 模块**：通过 2×2 像素重排（pixel-unshuffle）聚合视觉令牌，在传递给大语言模型前完成高效预处理，平衡计算效率与任务性能。

#### 2.2 **Two-Stage Parsing Strategy**解耦式两阶段解析策略:
1. 第一阶段：将文档图像**下采样**至 1036×1036 像素，高效完成全局布局分析，识别标题、文本、表格等结构元素，避免直接处理高分辨率图像的 O (N²) 复杂度，同时固定尺寸设计提升边界框定位稳定性。
    输出标准化格式（.json）：
    * 位置：通过边界框（x1,y1,x2,y2）定位元素坐标。
    * 类别：区分标题、文本块、表格、公式、图像、页眉、页脚等 18 + 细粒度类别（基于统一标签体系）。
    * 旋转角度：判断元素是否旋转及旋转方向（上、下、左、右），为后续校正做准备。
    * 阅读顺序：预测元素的逻辑阅读顺序，避免解析结果杂乱。*
    输出格式示例：<<|box_start|>163 81 836 129<<|box_end|><<|ref_start|>table_caption<<|ref_end|><<|rotate_up|>
1. 第二阶段：基于布局结果，从原始图像中裁剪提取**原生分辨率**的局部区域，裁剪尺寸上限为 2048×28×28 像素（平衡细节保留与计算量），针对性进行文本、公式、表格的细粒度识别，保留关键细节。

#### 2.3 三阶段递进式训练
1. 阶段 0（模态对齐）：先通过图像 - 文本对训练 MLP 适配器，再经 VQA 数据微调，奠定视觉 - 语言对齐与 OCR 基础。

2. 阶段 1（预训练）：6.9M 样本构建布局分析、元素识别核心能力，任务特定提示词引导训练。

3. 阶段 2（微调）：630K 高质量样本 + 难例（IMIC 挖掘），优化复杂场景性能，保留基础能力。

