## 一、相关模型调研
测评效果的比对主要采用手头所有的经过挑选的有代表性的共44页6组投标文档，人工进行比对。
### 1.1  MonkeyOCR
 https://github.com/Yuliang-Liu/MonkeyOCR
#### 1.1.1  MonkeyOCR
Demo：http://vlrlabmonkey.xyz:8891
部分解析效果展示：
 
| ![MonkeyOCR](./pictures/Mo01.png)|![MonkeyOCR](./pictures/Mo02.png)|
|--|--|
 | ![MonkeyOCR](./pictures/Mo04.png)|![MonkeyOCR](./pictures/Mo03.png)|
| ![MonkeyOCR](./pictures/Mo05.png)|![MonkeyOCR](./pictures/Mo08.png)|
| ![MonkeyOCR](./pictures/Mo07.png)|![MonkeyOCR](./pictures/Mo06.png)|
 
平均解析速度为3.2s/页
MonkeyOCR在用以测试的投标文件中表现较差，出现内容缺失，页眉跨页表格错误分类，表格覆盖不全的情况，问题较多，除了解析速度并未发现其他比MinerU2.5的vlm解析效果好的地方，故不予采用。
#### 1.1.2  MonkeyOCR v1.5
Demo:https://aiwrite.wps.cn/pdf/parse/web/
11月中旬出的，解析效果较MonkeyOCR有了很大提升，在页眉标题识别这一块的效果优于MinerU2.5和Dolphin v2。
部分解析效果展示（2026.01.22重新测评）：
| ![MonkeyOCR v1.5](./pictures/Mo3.png)|![MonkeyOCR v1.5](./pictures/Mo8.png)|
|--|--|
| ![MonkeyOCR v1.5](./pictures/Mo7.png)|![MonkeyOCR v1.5](./pictures/Mo2.png)|
| ![MonkeyOCR v1.5](./pictures/Mo4.png)|![MonkeyOCR v1.5](./pictures/Mo5.png)|
| ![MonkeyOCR v1.5](./pictures/Mo10.png)|![MonkeyOCR v1.5](./pictures/Mo11.png)|
| ![MonkeyOCR v1.5](./pictures/Mo9.png)|![MonkeyOCR v1.5](./pictures/Mo1.png)|
| ![MonkeyOCR v1.5](./pictures/Mo6.png)|

平均解析速度为3.1s/页
分类效果除表格外都没出问题，边界框覆盖很全面，整体效果比MinerU2.5的vlm还好，但跨页表格分类还是不准确，不能进行跨页合并。
MonkeyOCR v1.5出来时demo和代码似乎未更新完全，当时测验效果跟MoonkeyOCR一致，故后续优化MinerU2.5时未考虑该模型。
### 1.2 FluxOCR
https://github.com/chatdoc-com/OCRFlux
Demo：https://ocrflux.pdfparser.io/#/
简单展示用Demo测验的效果。Demo只支持3页，且每日解析数量有上限，所以体现的问题并不全面。
部分解析效果展示：
| ![FluxOCR](./pictures/F1.png)|![FluxOCR](./pictures/F2.png)|
|--|--|
|![FluxOCR](./pictures/F3.png)|![FluxOCR](./pictures/F4.png)|

解析速度也在3s/页，跨页表格、页眉分类错误。跨页表格合并效果不错，但有时又不能合并。

###  1.3  Logics-Parsing 
https://github.com/alibaba/Logics-Parsing
Demo：https://www.modelscope.cn/studios/Alibaba-DT/Logics-Parsing/summary
阿里的，处理速度和MinerU差不多，效果整体看起来并不比MinerU2.5好，文章里写的也是比MinerU2好

27s 10
28s 14
16s 1
23s 10
17s 5
21s 10
2.  技术原理:
* 基于 Qwen2.5-VL-7B：Logics-Parsing 以强大的 Qwen2.5-VL-7B 模型为基础，继承了其在视觉和语言处理方面的优势。
* 两阶段训练：第一阶段是监督微调，模型学习生成结构化的 HTML 输出；第二阶段是布局为中心的强化学习，通过文本准确性、布局定位和阅读顺序三个奖励组件优化模型。
* 强化学习优化：通过强化学习，模型能更好地理解文档的布局和内容的逻辑顺序，生成更准确的结构化输出。
*  结构化 HTML 输出：模型能将文档图像转换为结构化的 HTML 格式，保留文档的逻辑结构，每个内容块都有类别、边界框坐标和 OCR 文本标签。
* 高级内容识别：模型能准确识别复杂科学公式、化学结构和手写中文字符，将化学结构转换为标准的 SMILES 格式。
* 自动去除无关元素：模型能自动识别并过滤掉页眉、页脚等无关元素，专注于文档的核心内容。

## MinerU2.5
#### **Two-Stage Parsing Strategy**解耦式两阶段解析策略:
1. 第一阶段：将文档图像下采样至 1036×1036 像素，高效完成全局布局分析，识别标题、文本、表格等结构元素，规避高分辨率处理的计算冗余。
2. 第二阶段：基于布局结果，从原始图像中提取原生分辨率的局部区域，针对性进行文本、公式、表格的细粒度识别，保留关键细节。
视觉编码器初始化自Qwen2-VL-2B-Instruct，而语言模型初始化自Qwen2-Instruct-0.5B。